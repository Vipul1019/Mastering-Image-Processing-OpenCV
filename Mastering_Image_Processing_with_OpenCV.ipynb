{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**üîπStep 1: Load and Display an Image**"
      ],
      "metadata": {
        "id": "gxk9BHRz8OQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1Ô∏è‚É£ Upload an Image to Colab\n",
        "\n",
        "Since Google Colab runs on the cloud, you need to upload an image before reading it."
      ],
      "metadata": {
        "id": "JKWnHZ5aPPIH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa14GuHv7vaz"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ This will prompt you to select an image from your system.\n",
        "\n",
        "‚úÖ After uploading, the image will be saved in the Colab session."
      ],
      "metadata": {
        "id": "zDr0CRKcRjcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2Ô∏è‚É£ Read & Display the Image Using OpenCV"
      ],
      "metadata": {
        "id": "ESOugHnqPuvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Read the uploaded image (Replace 'your_image.jpg' with the actual filename)\n",
        "image_path = list(uploaded.keys())[0]  # Get the uploaded file name\n",
        "image = cv2.imread(image_path)  # Read the image\n",
        "\n",
        "# Convert BGR to RGB (since OpenCV loads images in BGR format)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display the image using Matplotlib\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis(\"off\")  # Hide axes\n",
        "plt.title(\"Original Image\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l_SiWLBDPTOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Key Takeaways:\n",
        "\n",
        "1.   cv2.imread(image_path) ‚Üí Reads the image.\n",
        "2.   OpenCV loads images in BGR format, while Matplotlib expects RGB.\n",
        "3. cv2.cvtColor(image, cv2.COLOR_BGR2RGB) ‚Üí Converts BGR to RGB.\n",
        "4. plt.imshow(image_rgb) ‚Üí Displays the image in Colab.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n2-3hDYVP2Nn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "3flQk8b9_Hp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ Step 2: Print Image Properties (Shape, Size, Data Type)**"
      ],
      "metadata": {
        "id": "WCJ6YvHZQSwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print image properties\n",
        "print(f\"Image Shape: {image.shape}\")  # (Height, Width, Channels)\n",
        "print(f\"Image Size (Total Pixels): {image.size}\")\n",
        "print(f\"Image Data Type: {image.dtype}\")"
      ],
      "metadata": {
        "id": "xA0lct_nQeKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What do these properties mean?\n",
        "\n",
        "1. Shape ‚Üí (Height, Width, Channels), where Channels = 3 for RGB images.\n",
        "2. Size ‚Üí Total number of pixels (Height √ó Width √ó Channels).\n",
        "3. Data Type ‚Üí Pixel value storage type (usually uint8)."
      ],
      "metadata": {
        "id": "PgF5uTckQhwO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "qpURpQNZ_Lpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ Step 3: Convert Image to Grayscale**"
      ],
      "metadata": {
        "id": "2tlzTTSDQtq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert image to grayscale\n",
        "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Display grayscale image\n",
        "plt.imshow(gray_image, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Grayscale Image\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dULP4BN6QxG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Why Convert to Grayscale?\n",
        "\n",
        "1. Reduces computational cost (Single channel instead of 3).\n",
        "2. Used in many computer vision tasks (edge detection, segmentation)."
      ],
      "metadata": {
        "id": "Xh1Vr3gCQ1Ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "1YdNqLbC_PmC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ Step 4: Display Color & Grayscale Images Side by Side**"
      ],
      "metadata": {
        "id": "LWTA2OoGRGbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with two subplots (1 row, 2 columns)\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Original Color Image (RGB)\n",
        "plt.subplot(1, 2, 1)  # 1 row, 2 columns, position 1\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original (RGB)\")\n",
        "\n",
        "# Grayscale Image\n",
        "plt.subplot(1, 2, 2)  # 1 row, 2 columns, position 2\n",
        "plt.imshow(gray_image, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Grayscale\")\n",
        "\n",
        "# Show the images\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1dfMmbABRJGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What Happens Here?\n",
        "1. plt.subplot(1, 2, 1) ‚Üí First subplot (Original Image).\n",
        "2. plt.subplot(1, 2, 2) ‚Üí Second subplot (Grayscale Image).\n",
        "3. plt.figure(figsize=(10,5)) ‚Üí Adjusts figure size for better visibility.\n"
      ],
      "metadata": {
        "id": "nlmtdJQtRND1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n"
      ],
      "metadata": {
        "id": "0jlCmNvv_SPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ Step 5: Resize the Image**"
      ],
      "metadata": {
        "id": "WYnAUa_KRsn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resize image to half its original size\n",
        "height, width = image.shape[:2]  # Get original height and width\n",
        "resized_half = cv2.resize(image, (width // 2, height // 2))\n",
        "\n",
        "# Resize image to double its original size\n",
        "resized_double = cv2.resize(image, (width * 2, height * 2))\n",
        "\n",
        "# Resize image to a fixed size (e.g., 300x300)\n",
        "resized_fixed = cv2.resize(image, (300, 300))\n",
        "\n",
        "# Convert to RGB for correct display\n",
        "resized_half_rgb = cv2.cvtColor(resized_half, cv2.COLOR_BGR2RGB)\n",
        "resized_double_rgb = cv2.cvtColor(resized_double, cv2.COLOR_BGR2RGB)\n",
        "resized_fixed_rgb = cv2.cvtColor(resized_fixed, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display original and resized images\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original\")\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.imshow(resized_half_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Half Size\")\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.imshow(resized_double_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Double Size\")\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.imshow(resized_fixed_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Fixed (300x300)\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qRG8qigZRue9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What‚Äôs Happening Here?\n",
        "1. Original Size: Image stays the same.\n",
        "2. Half Size: cv2.resize(image, (width // 2, height // 2)) scales it down by 50%.\n",
        "3. Double Size: cv2.resize(image, (width * 2, height * 2)) enlarges it by 2x.\n",
        "4. Fixed Size (300x300): Forces the image to 300√ó300 pixels.\n",
        "5. Matplotlib Subplots: Show all versions side by side for easy comparison."
      ],
      "metadata": {
        "id": "6Ut-AKPRRz3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n"
      ],
      "metadata": {
        "id": "OzJTMKNb_UQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ Step 6: Flip the Image (Mirror Effect)**\n",
        "\n",
        "üìå Flipping can be done in three ways:\n",
        "\n",
        "1. Horizontally (Left-Right) ‚Üí flipCode = 1\n",
        "2. Vertically (Top-Bottom) ‚Üí flipCode = 0\n",
        "3. Both Directions (Rotate 180¬∞) ‚Üí flipCode = -1\n"
      ],
      "metadata": {
        "id": "l_5D5n5hSCrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flip Horizontally (Left-Right)\n",
        "flipped_horizontally = cv2.flip(image, 1)\n",
        "\n",
        "# Flip Vertically (Top-Bottom)\n",
        "flipped_vertically = cv2.flip(image, 0)\n",
        "\n",
        "# Flip Both (Rotate 180¬∞)\n",
        "flipped_both = cv2.flip(image, -1)\n",
        "\n",
        "# Convert BGR to RGB for correct display\n",
        "flipped_horizontally_rgb = cv2.cvtColor(flipped_horizontally, cv2.COLOR_BGR2RGB)\n",
        "flipped_vertically_rgb = cv2.cvtColor(flipped_vertically, cv2.COLOR_BGR2RGB)\n",
        "flipped_both_rgb = cv2.cvtColor(flipped_both, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display original and flipped images\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original\")\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.imshow(flipped_horizontally_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Flipped Horizontally\")\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.imshow(flipped_vertically_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Flipped Vertically\")\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.imshow(flipped_both_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Flipped Both\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cBw4gr87SOBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What‚Äôs Happening Here?\n",
        "1. Horizontal Flip (flipCode=1) ‚Üí Left and right sides are swapped.\n",
        "2. Vertical Flip (flipCode=0) ‚Üí The image is flipped upside down.\n",
        "3. Both Flip (flipCode=-1) ‚Üí Rotates the image 180¬∞ (both horizontal & vertical flip).\n",
        "4. Matplotlib shows all versions side by side for easy visualization."
      ],
      "metadata": {
        "id": "SoaS1b-mSTK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n"
      ],
      "metadata": {
        "id": "gOrOjf3U_V8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ Step 7: Rotate the Image at Different Angles**"
      ],
      "metadata": {
        "id": "OeKWfFWhTOcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get image dimensions\n",
        "(h, w) = image.shape[:2]\n",
        "\n",
        "# Define rotation matrices\n",
        "rotate_90 = cv2.getRotationMatrix2D((w // 2, h // 2), 90, 1.0)\n",
        "rotate_180 = cv2.getRotationMatrix2D((w // 2, h // 2), 180, 1.0)\n",
        "rotate_270 = cv2.getRotationMatrix2D((w // 2, h // 2), 270, 1.0)\n",
        "\n",
        "# Apply rotation\n",
        "rotated_90 = cv2.warpAffine(image, rotate_90, (w, h))\n",
        "rotated_180 = cv2.warpAffine(image, rotate_180, (w, h))\n",
        "rotated_270 = cv2.warpAffine(image, rotate_270, (w, h))\n",
        "\n",
        "# Convert to RGB for display\n",
        "rotated_90_rgb = cv2.cvtColor(rotated_90, cv2.COLOR_BGR2RGB)\n",
        "rotated_180_rgb = cv2.cvtColor(rotated_180, cv2.COLOR_BGR2RGB)\n",
        "rotated_270_rgb = cv2.cvtColor(rotated_270, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display original and rotated images\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original\")\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.imshow(rotated_90_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Rotated 90¬∞\")\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.imshow(rotated_180_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Rotated 180¬∞\")\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.imshow(rotated_270_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Rotated 270¬∞\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5CE4dM-XTP-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What‚Äôs Happening Here?\n",
        "1. cv2.getRotationMatrix2D() ‚Üí Creates a transformation matrix for rotation.\n",
        "2. cv2.warpAffine() ‚Üí Applies the transformation to rotate the image.\n",
        "3. Each image is rotated around the center by 90¬∞, 180¬∞, and 270¬∞.\n",
        "4. Matplotlib displays all versions side by side for easy visualization."
      ],
      "metadata": {
        "id": "7sCL9t6FTT-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n"
      ],
      "metadata": {
        "id": "iXBEMyrU_XY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ Step 8: Crop a Region of Interest (ROI)**\n",
        "\n",
        "üìå Cropping an image means selecting a specific portion of it (e.g., a face in an image).\n",
        "\n",
        "üìå We will manually define coordinates for cropping."
      ],
      "metadata": {
        "id": "d8fgSJY9TuVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define cropping coordinates (adjust as needed)\n",
        "x_start, y_start = 100, 50   # Top-left corner (x, y)\n",
        "x_end, y_end = 400, 300      # Bottom-right corner (x, y)\n",
        "\n",
        "# Crop the image using NumPy slicing\n",
        "cropped_image = image[y_start:y_end, x_start:x_end]\n",
        "\n",
        "# Convert to RGB for correct display\n",
        "cropped_image_rgb = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display original and cropped images\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original Image\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(cropped_image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Cropped Image\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WnpqK57RT4lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What‚Äôs Happening Here?\n",
        "1. We define a rectangular region (x_start:x_end, y_start:y_end).\n",
        "2. NumPy slicing extracts that part of the image (image[y1:y2, x1:x2]).\n",
        "3. We display both the original and cropped images side by side for comparison."
      ],
      "metadata": {
        "id": "ZB86ddfqT_7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n"
      ],
      "metadata": {
        "id": "RtvRP_fg_Ypl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ Step 9: Apply Blurring Filters**\n",
        "\n",
        "üìå Blurring is useful for reducing noise and smoothing images.\n",
        "\n",
        "üìå We will apply different blurring techniques:\n",
        "\n",
        "1Ô∏è‚É£ Averaging Blur ‚Äì Simple blur by averaging pixel values.\n",
        "\n",
        "2Ô∏è‚É£ Gaussian Blur ‚Äì Softens edges while maintaining structures.\n",
        "\n",
        "3Ô∏è‚É£ Median Blur ‚Äì Best for noise reduction (salt-and-pepper noise).\n",
        "\n",
        "4Ô∏è‚É£ Bilateral Filter ‚Äì Preserves edges while blurring."
      ],
      "metadata": {
        "id": "WimFYFDPWYrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå 1Ô∏è‚É£ Apply Different Blur Filters"
      ],
      "metadata": {
        "id": "9JyCyr5oWjyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply different types of blurring\n",
        "average_blur = cv2.blur(image, (5, 5))  # Averaging Blur\n",
        "gaussian_blur = cv2.GaussianBlur(image, (5, 5), 0)  # Gaussian Blur\n",
        "median_blur = cv2.medianBlur(image, 5)  # Median Blur\n",
        "bilateral_blur = cv2.bilateralFilter(image, 9, 75, 75)  # Bilateral Filter\n",
        "\n",
        "# Convert to RGB for correct display\n",
        "average_blur_rgb = cv2.cvtColor(average_blur, cv2.COLOR_BGR2RGB)\n",
        "gaussian_blur_rgb = cv2.cvtColor(gaussian_blur, cv2.COLOR_BGR2RGB)\n",
        "median_blur_rgb = cv2.cvtColor(median_blur, cv2.COLOR_BGR2RGB)\n",
        "bilateral_blur_rgb = cv2.cvtColor(bilateral_blur, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display original and blurred images\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original\")\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.imshow(average_blur_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Averaging Blur\")\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.imshow(gaussian_blur_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Gaussian Blur\")\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.imshow(median_blur_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Median Blur\")\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.imshow(bilateral_blur_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Bilateral Filter\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "34gki8zPWmPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What‚Äôs Happening Here?\n",
        "1. Averaging Blur (cv2.blur()) ‚Üí Averages nearby pixel values.\n",
        "2. Gaussian Blur (cv2.GaussianBlur()) ‚Üí Uses a Gaussian function for a smooth blur.\n",
        "3. Median Blur (cv2.medianBlur()) ‚Üí Uses the median of surrounding pixels (best for noise removal).\n",
        "4. Bilateral Filter (cv2.bilateralFilter()) ‚Üí Reduces noise while preserving edges."
      ],
      "metadata": {
        "id": "te06O52ZWpdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n"
      ],
      "metadata": {
        "id": "EK7MMwWT_bX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ Step 10: Edge Detection Using Canny**\n",
        "\n",
        "üìå Edge detection is useful for identifying object boundaries in images.\n",
        "\n",
        "üìå Canny Edge Detector is one of the most popular techniques."
      ],
      "metadata": {
        "id": "HWos60lMW4B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert image to grayscale (Edge detection works best in grayscale)\n",
        "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply Canny Edge Detection\n",
        "edges_weak = cv2.Canny(gray_image, 50, 150)  # Low threshold (detects more edges)\n",
        "edges_strong = cv2.Canny(gray_image, 100, 200)  # High threshold (detects fewer edges)\n",
        "\n",
        "# Display original and edge-detected images\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(gray_image, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Grayscale Image\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(edges_weak, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Edges (Low Threshold)\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(edges_strong, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Edges (High Threshold)\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wTvfYv14W97o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What‚Äôs Happening Here?\n",
        "1. Convert image to grayscale ‚Üí Edge detection works best in grayscale images.\n",
        "2. cv2.Canny(image, threshold1, threshold2) ‚Üí Detects edges.\n",
        "\n",
        "     threshold1 (lower) ‚Üí Detects more edges.\n",
        "\n",
        "     threshold2 (higher) ‚Üí Detects fewer edges (only strong ones).\n",
        "\n",
        "3. Higher threshold detects stronger, more defined edges."
      ],
      "metadata": {
        "id": "gZqv28ELXCdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå There are also other edge detection methods. Please refer those aswell."
      ],
      "metadata": {
        "id": "tX2EbsGOAgwk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n"
      ],
      "metadata": {
        "id": "ZLNyUAcz_c64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ Step 11: Morphological Operations (Dilation & Erosion)**\n",
        "\n",
        "üìå Morphological operations are useful for enhancing edges, removing noise, and improving object structures in images.\n",
        "\n",
        "üìå Common operations include:\n",
        "\n",
        "1Ô∏è‚É£ Erosion ‚Üí Shrinks objects in an image (removes noise).\n",
        "\n",
        "2Ô∏è‚É£ Dilation ‚Üí Expands objects in an image (fills gaps).\n",
        "\n",
        "3Ô∏è‚É£ Opening (Erosion ‚Üí Dilation) ‚Üí Removes small noise.\n",
        "\n",
        "4Ô∏è‚É£ Closing (Dilation ‚Üí Erosion) ‚Üí Closes small holes/gaps in objects.\n",
        "\n"
      ],
      "metadata": {
        "id": "b3O-N444Xb5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a kernel (structuring element) - a 5x5 matrix of ones\n",
        "kernel = np.ones((5, 5), np.uint8)\n",
        "\n",
        "# Apply morphological operations\n",
        "eroded = cv2.erode(edges_strong, kernel, iterations=1)  # Erosion\n",
        "dilated = cv2.dilate(edges_strong, kernel, iterations=1)  # Dilation\n",
        "opening = cv2.morphologyEx(edges_strong, cv2.MORPH_OPEN, kernel)  # Opening\n",
        "closing = cv2.morphologyEx(edges_strong, cv2.MORPH_CLOSE, kernel)  # Closing\n",
        "\n",
        "# Display original and transformed images\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.imshow(edges_strong, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original Edges\")\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.imshow(eroded, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Erosion (Removes Noise)\")\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.imshow(dilated, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Dilation (Enhances Edges)\")\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.imshow(opening, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Opening (Erosion ‚Üí Dilation)\")\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.imshow(closing, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Closing (Dilation ‚Üí Erosion)\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7UpbspVjXkHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What‚Äôs Happening Here?\n",
        "1. Erosion (cv2.erode()) ‚Üí Shrinks bright areas, removes small noise.\n",
        "2. Dilation (cv2.dilate()) ‚Üí Expands bright areas, strengthens edges.\n",
        "3. Opening (cv2.MORPH_OPEN) ‚Üí First erosion, then dilation (removes small noise).\n",
        "4. Closing (cv2.MORPH_CLOSE) ‚Üí First dilation, then erosion (closes small holes)."
      ],
      "metadata": {
        "id": "qRqaMXuqXmnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>\n"
      ],
      "metadata": {
        "id": "RXaPooIz_fl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ Step 12: Image Thresholding (Binary & Adaptive)**\n",
        "\n",
        "üìå Thresholding is used to convert grayscale images into binary images (black & white).\n",
        "\n",
        "üìå Common thresholding techniques include:\n",
        "\n",
        "1Ô∏è‚É£ Simple Binary Thresholding ‚Üí Pixels above a threshold are set to white, others to black.\n",
        "\n",
        "2Ô∏è‚É£ Inverse Binary Thresholding ‚Üí Opposite of binary thresholding.\n",
        "\n",
        "3Ô∏è‚É£ Adaptive Thresholding ‚Üí Threshold varies across the image, useful for uneven lighting.\n",
        "\n",
        "4Ô∏è‚É£ Otsu‚Äôs Thresholding ‚Üí Automatically determines the best threshold value."
      ],
      "metadata": {
        "id": "BKOLzAYMXwzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert image to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply different thresholding methods\n",
        "_, binary_thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)  # Simple Binary\n",
        "_, binary_inv_thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)  # Inverse Binary\n",
        "adaptive_thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                        cv2.THRESH_BINARY, 11, 2)  # Adaptive Threshold\n",
        "_, otsu_thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)  # Otsu's Method\n",
        "\n",
        "# Display results\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.imshow(gray, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original Grayscale\")\n",
        "\n",
        "plt.subplot(2, 3, 2)\n",
        "plt.imshow(binary_thresh, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Binary Threshold\")\n",
        "\n",
        "plt.subplot(2, 3, 3)\n",
        "plt.imshow(binary_inv_thresh, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Inverse Binary Threshold\")\n",
        "\n",
        "plt.subplot(2, 3, 4)\n",
        "plt.imshow(adaptive_thresh, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Adaptive Threshold\")\n",
        "\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.imshow(otsu_thresh, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Otsu's Thresholding\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gchPcQESX3m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What‚Äôs Happening Here?\n",
        "1. Binary Thresholding (cv2.THRESH_BINARY) ‚Üí Pixels above 127 become white, others become black.\n",
        "2. Inverse Binary Thresholding (cv2.THRESH_BINARY_INV) ‚Üí Opposite effect.\n",
        "3. Adaptive Thresholding (cv2.adaptiveThreshold()) ‚Üí Works well for images with varying lighting conditions.\n",
        "4. Otsu‚Äôs Thresholding (cv2.THRESH_OTSU) ‚Üí Automatically selects the best threshold value."
      ],
      "metadata": {
        "id": "jwOnylyrX-hI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "Qjbc7ygX_i6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ Step 13: Contour Detection & Shape Analysis**\n",
        "\n",
        "üìå Contours are curves that join continuous points along the boundary of objects in an image.\n",
        "\n",
        "üìå Contours are used for shape analysis, object detection, and recognition.\n",
        "\n",
        "üìå We will:\n",
        "\n",
        "1Ô∏è‚É£ Detect and draw contours on objects.\n",
        "\n",
        "2Ô∏è‚É£ Find the largest contour.\n",
        "\n",
        "3Ô∏è‚É£ Approximate contour shapes (Rectangle, Circle, etc.).\n",
        "\n"
      ],
      "metadata": {
        "id": "1VVutgEoYLAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå 1Ô∏è‚É£ Detect and Draw Contours"
      ],
      "metadata": {
        "id": "woTC11irYgiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Convert to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply Canny Edge Detection\n",
        "edges = cv2.Canny(gray, 50, 150)\n",
        "\n",
        "# Find contours\n",
        "contours, _ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Draw contours on a copy of the original image\n",
        "contour_image = image.copy()\n",
        "cv2.drawContours(contour_image, contours, -1, (0, 255, 0), 2)  # Green contours\n",
        "\n",
        "# Convert to RGB for display\n",
        "contour_image_rgb = cv2.cvtColor(contour_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display original and contour-detected images\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original Image\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(contour_image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Detected Contours\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n1clKfM_YS9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå 2Ô∏è‚É£ Find and Draw the Largest Contour"
      ],
      "metadata": {
        "id": "3ThkcDFyYjkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the largest contour\n",
        "largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "# Draw only the largest contour\n",
        "largest_contour_image = image.copy()\n",
        "cv2.drawContours(largest_contour_image, [largest_contour], -1, (255, 0, 0), 3)  # Blue contour\n",
        "\n",
        "# Convert to RGB for display\n",
        "largest_contour_image_rgb = cv2.cvtColor(largest_contour_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display result\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(largest_contour_image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Largest Contour\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HuOZlGOQYkBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå 3Ô∏è‚É£ Approximate Shape of Contours"
      ],
      "metadata": {
        "id": "xEt92wEMYo6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Approximate contours as polygons\n",
        "shape_image = image.copy()\n",
        "\n",
        "for contour in contours:\n",
        "    epsilon = 0.02 * cv2.arcLength(contour, True)  # Approximation factor\n",
        "    approx = cv2.approxPolyDP(contour, epsilon, True)  # Approximate contour\n",
        "\n",
        "    # Draw the approximated shape\n",
        "    cv2.drawContours(shape_image, [approx], -1, (0, 255, 255), 2)  # Yellow color\n",
        "\n",
        "# Convert to RGB for display\n",
        "shape_image_rgb = cv2.cvtColor(shape_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display result\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(shape_image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Approximated Shapes\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j9N9z7NuYrCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What‚Äôs Happening Here?\n",
        "1. Contour Detection (cv2.findContours()) ‚Üí Detects object boundaries.\n",
        "2. Drawing Contours (cv2.drawContours()) ‚Üí Draws detected contours on an image.\n",
        "3. Finding Largest Contour (max(contours, key=cv2.contourArea)) ‚Üí Identifies the biggest object.\n",
        "4. Approximating Contours (cv2.approxPolyDP()) ‚Üí Approximates shapes (triangle, rectangle, etc.).\n"
      ],
      "metadata": {
        "id": "N7_bFI90Yv4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "D_I-i_nT_mA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ Step 14: Histogram Analysis & Equalization**\n",
        "\n",
        "üìå Histograms represent the distribution of pixel intensities in an image.\n",
        "\n",
        "üìå Histogram Equalization enhances contrast by spreading out pixel intensity values.\n",
        "\n",
        "üìå We will:\n",
        "\n",
        "1Ô∏è‚É£ Plot image histograms (Grayscale & Color).\n",
        "\n",
        "2Ô∏è‚É£ Apply Histogram Equalization for contrast enhancement."
      ],
      "metadata": {
        "id": "vSyoE_qrY8hO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå 1Ô∏è‚É£ Plot Grayscale & Color Histograms"
      ],
      "metadata": {
        "id": "kKVjRdtmZIMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert image to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Compute histogram for grayscale image\n",
        "hist_gray = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
        "\n",
        "# Compute histograms for color channels\n",
        "colors = ('b', 'g', 'r')\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for i, color in enumerate(colors):\n",
        "    hist_color = cv2.calcHist([image], [i], None, [256], [0, 256])\n",
        "    plt.plot(hist_color, color=color, label=f'{color.upper()} Channel')\n",
        "\n",
        "# Plot grayscale histogram\n",
        "plt.plot(hist_gray, color='black', linestyle='dashed', label=\"Grayscale\")\n",
        "plt.legend()\n",
        "plt.title(\"Histogram of Image (Grayscale & Color Channels)\")\n",
        "plt.xlabel(\"Pixel Intensity\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n6oewNiGZF83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå 2Ô∏è‚É£ Apply Histogram Equalization (Contrast Enhancement)"
      ],
      "metadata": {
        "id": "fVCrIWwBZLv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Histogram Equalization\n",
        "equalized_gray = cv2.equalizeHist(gray)\n",
        "\n",
        "# Display original and equalized images\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(gray, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original Grayscale Image\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(equalized_gray, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Histogram Equalized Image\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FrxSkgi1ZMfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå 3Ô∏è‚É£ Histogram Equalization for Color Images (CLAHE)\n",
        "\n",
        "üìå CLAHE (Contrast Limited Adaptive Histogram Equalization) improves contrast while reducing noise."
      ],
      "metadata": {
        "id": "0PykbAgaZRgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to LAB color space\n",
        "lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
        "\n",
        "# Split LAB channels\n",
        "l, a, b = cv2.split(lab_image)\n",
        "\n",
        "# Apply CLAHE to L-channel\n",
        "clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
        "l_clahe = clahe.apply(l)\n",
        "\n",
        "# Merge channels back\n",
        "lab_clahe = cv2.merge((l_clahe, a, b))\n",
        "image_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
        "\n",
        "# Convert to RGB for correct display\n",
        "image_clahe_rgb = cv2.cvtColor(image_clahe, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display results\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original Image\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(image_clahe_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"CLAHE Enhanced Image\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YuubyTdEZSOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What‚Äôs Happening Here?\n",
        "1. Grayscale Histogram (cv2.calcHist()) ‚Üí Shows pixel intensity distribution.\n",
        "2. Color Histograms (cv2.calcHist() for RGB) ‚Üí Shows pixel distribution for each channel.\n",
        "3. Histogram Equalization (cv2.equalizeHist()) ‚Üí Spreads pixel intensities for better contrast.\n",
        "4. CLAHE (cv2.createCLAHE()) ‚Üí Adaptive equalization for better results in color images."
      ],
      "metadata": {
        "id": "eUQJiDoqZeWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "S2Y6FqZF_oqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ Step 15: Image Segmentation (Watershed & K-Means Clustering)**\n",
        "\n",
        "üìå Image segmentation is the process of dividing an image into meaningful regions or objects.\n",
        "\n",
        "üìå Common segmentation techniques:\n",
        "\n",
        "1Ô∏è‚É£ Threshold-based Segmentation (Already covered)\n",
        "\n",
        "2Ô∏è‚É£ Watershed Algorithm ‚Üí Used for separating overlapping objects.\n",
        "\n",
        "3Ô∏è‚É£ K-Means Clustering ‚Üí Groups similar pixels into clusters (unsupervised learning)."
      ],
      "metadata": {
        "id": "ot95bnLlZqfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå 1Ô∏è‚É£ Segmentation Using the Watershed Algorithm\n",
        "\n",
        "Watershed is used for object segmentation, especially when objects are touching."
      ],
      "metadata": {
        "id": "84uWHIcgZwKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert image to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Apply Otsu‚Äôs thresholding\n",
        "_, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "\n",
        "# Noise removal using morphological operations\n",
        "kernel = np.ones((3, 3), np.uint8)\n",
        "opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
        "\n",
        "# Sure background (dilated image)\n",
        "sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
        "\n",
        "# Sure foreground (distance transform)\n",
        "dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
        "_, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
        "\n",
        "# Finding unknown region\n",
        "sure_fg = np.uint8(sure_fg)\n",
        "unknown = cv2.subtract(sure_bg, sure_fg)\n",
        "\n",
        "# Marker labelling\n",
        "_, markers = cv2.connectedComponents(sure_fg)\n",
        "markers = markers + 1  # Ensure background is not zero\n",
        "markers[unknown == 255] = 0  # Mark the unknown region with 0\n",
        "\n",
        "# Apply watershed algorithm\n",
        "watershed_image = image.copy()\n",
        "cv2.watershed(watershed_image, markers)\n",
        "watershed_image[markers == -1] = [255, 0, 0]  # Mark boundaries in red\n",
        "\n",
        "# Convert to RGB for correct display\n",
        "watershed_image_rgb = cv2.cvtColor(watershed_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display results\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(gray, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Grayscale Image\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(thresh, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Thresholded Image\")\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(watershed_image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Segmented Image (Watershed)\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "07H1AOlTZ2Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå 2Ô∏è‚É£ Segmentation Using K-Means Clustering\n",
        "K-Means clustering groups similar pixels into clusters."
      ],
      "metadata": {
        "id": "-vTgIKF_Z551"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert image to RGB (K-Means works in RGB)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Reshape image into 2D array of pixels (each pixel is a 3D vector)\n",
        "pixel_values = image_rgb.reshape((-1, 3))\n",
        "pixel_values = np.float32(pixel_values)\n",
        "\n",
        "# Define K-Means criteria (type, max iterations, accuracy)\n",
        "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.2)\n",
        "\n",
        "# Number of clusters (K)\n",
        "K = 3\n",
        "\n",
        "# Apply K-Means clustering\n",
        "_, labels, centers = cv2.kmeans(pixel_values, K, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
        "\n",
        "# Convert centers to uint8 (RGB format)\n",
        "centers = np.uint8(centers)\n",
        "\n",
        "# Map labels to center colors\n",
        "segmented_image = centers[labels.flatten()]\n",
        "segmented_image = segmented_image.reshape(image_rgb.shape)\n",
        "\n",
        "# Display original and segmented images\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original Image\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(segmented_image)\n",
        "plt.axis(\"off\")\n",
        "plt.title(f\"Segmented Image (K={K} Clusters)\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b3MbgWH8Z7ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What‚Äôs Happening Here?\n",
        "1. Watershed Algorithm (cv2.watershed()) ‚Üí Separates overlapping objects.\n",
        "2. K-Means Clustering (cv2.kmeans()) ‚Üí Groups similar colors into K clusters.\n",
        "3. Morphological Operations (cv2.morphologyEx()) ‚Üí Removes noise before segmentation.\n",
        "4. Distance Transform (cv2.distanceTransform()) ‚Üí Helps in defining foreground & background regions."
      ],
      "metadata": {
        "id": "1TyGg-lYZ9tt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "GMPUO4Vt_qaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ Step 16: Face Detection Using OpenCV & Haar Cascades**\n",
        "\n",
        "üìå Face detection is a computer vision task that identifies human faces in images or videos.\n",
        "\n",
        "üìå Haar Cascades are pre-trained classifiers that detect faces, eyes, and other objects.\n",
        "\n",
        "üìå We will:\n",
        "\n",
        "1Ô∏è‚É£ Load a pre-trained Haar Cascade model.\n",
        "\n",
        "2Ô∏è‚É£ Detect faces in an image.\n",
        "\n",
        "3Ô∏è‚É£ Draw bounding boxes around detected faces."
      ],
      "metadata": {
        "id": "oMiEYv9vaLZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå 1Ô∏è‚É£ Load Haar Cascade & Detect Faces"
      ],
      "metadata": {
        "id": "5FUXNeRpaSWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained Haar Cascade for face detection\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "# Convert image to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Detect faces\n",
        "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "# Draw bounding boxes around detected faces\n",
        "face_detected_image = image.copy()\n",
        "for (x, y, w, h) in faces:\n",
        "    cv2.rectangle(face_detected_image, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green box\n",
        "\n",
        "# Convert to RGB for correct display\n",
        "face_detected_image_rgb = cv2.cvtColor(face_detected_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display results\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Original Image\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(face_detected_image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Face Detection\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WVOtxcQcaTF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå 2Ô∏è‚É£ Detect Eyes Along with Faces"
      ],
      "metadata": {
        "id": "sM_GaoHbaWI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Haar Cascade for eyes\n",
        "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
        "\n",
        "# Copy image for drawing\n",
        "eye_detected_image = image.copy()\n",
        "\n",
        "# Detect eyes within detected faces\n",
        "for (x, y, w, h) in faces:\n",
        "    roi_gray = gray[y:y + h, x:x + w]  # Region of interest for eyes\n",
        "    roi_color = eye_detected_image[y:y + h, x:x + w]\n",
        "\n",
        "    # Detect eyes\n",
        "    eyes = eye_cascade.detectMultiScale(roi_gray, scaleFactor=1.1, minNeighbors=10, minSize=(15, 15))\n",
        "\n",
        "    # Draw bounding boxes around eyes\n",
        "    for (ex, ey, ew, eh) in eyes:\n",
        "        cv2.rectangle(roi_color, (ex, ey), (ex + ew, ey + eh), (255, 0, 0), 2)  # Blue box\n",
        "\n",
        "# Convert to RGB for correct display\n",
        "eye_detected_image_rgb = cv2.cvtColor(eye_detected_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display results\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(eye_detected_image_rgb)\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Face & Eye Detection\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ECEsv_WUaW79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What‚Äôs Happening Here?\n",
        "1. Haar Cascades (cv2.CascadeClassifier()) ‚Üí Pre-trained models for object detection.\n",
        "2. Face Detection (detectMultiScale()) ‚Üí Detects faces based on features.\n",
        "3. Eye Detection (haarcascade_eye.xml) ‚Üí Detects eyes within the detected face.\n",
        "4. Bounding Boxes (cv2.rectangle()) ‚Üí Draws boxes around detected objects."
      ],
      "metadata": {
        "id": "Ue-5OrMEaaeN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "V_4Y89Jt_sIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**üîπ Step 17: Real-Time Face Detection Using OpenCV & Webcam**\n",
        "\n",
        "üìå We will use OpenCV to access the webcam and perform live face detection.\n",
        "\n",
        "üìå This will detect faces in real-time and display the results with bounding boxes."
      ],
      "metadata": {
        "id": "98utEJRYajQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå 1Ô∏è‚É£ Live Face Detection Using Webcam"
      ],
      "metadata": {
        "id": "OLYw85Q5anh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "# Load pre-trained Haar Cascade for face detection\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "# Start the webcam\n",
        "cap = cv2.VideoCapture(0)  # 0 is for the default webcam\n",
        "\n",
        "# Check if the webcam opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not open webcam.\")\n",
        "    exit()\n",
        "\n",
        "while True:\n",
        "    # Read a frame from the webcam\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Check if frame is successfully read\n",
        "    if not ret:\n",
        "        print(\"Error: Could not read frame from webcam.\")\n",
        "        break  # Exit the loop if frame reading fails\n",
        "\n",
        "    # Convert frame to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    # Draw bounding boxes around detected faces\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green box\n",
        "\n",
        "    # Show the video stream\n",
        "    cv2.imshow(\"Live Face Detection\", frame)\n",
        "\n",
        "    # Break the loop on pressing 'q'\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release the webcam and close windows\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "eJKcH9nUapH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ What‚Äôs Happening Here?\n",
        "1. Opens Webcam (cv2.VideoCapture(0)) ‚Üí Captures video in real-time.\n",
        "2. Face Detection (detectMultiScale()) ‚Üí Detects faces frame by frame.\n",
        "3. Bounding Boxes (cv2.rectangle()) ‚Üí Draws green boxes around detected faces.\n",
        "4. Breaks Loop (cv2.waitKey(1)) ‚Üí Stops detection when 'q' is pressed."
      ],
      "metadata": {
        "id": "bGnCeUThasmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br><br><br><br><br><br><br><br><br><br><br>"
      ],
      "metadata": {
        "id": "V4EJc3t1AyGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Homework:**\n",
        "\n",
        "You are given with a frames which has objects in them. For example, consider a pen. Detect the pen in the frame using any detection algorithm using Opencv.\n",
        "\n",
        "NOTE: Expected good accuracy(above 95%) and less Inference Time."
      ],
      "metadata": {
        "id": "76rSGWShA5yL"
      }
    }
  ]
}